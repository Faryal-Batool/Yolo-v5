{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTvDNSILZoN9"
      },
      "source": [
        "#clone YOLOv5 and\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Assemble Our Dataset\n",
        "\n",
        "In order to train our custom model, we need to assemble a dataset of representative images with bounding box annotations around the objects that we want to detect. And we need our dataset to be in YOLOv5 format.\n",
        "\n",
        "In Roboflow, you can choose between two paths:\n",
        "\n",
        "* Convert an existing dataset to YOLOv5 format. Roboflow supports over [30 formats object detection formats](https://roboflow.com/formats) for conversion.\n",
        "* Upload raw images and annotate them in Roboflow with [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "# Annotate\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
        "\n",
        "# Version\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2wGvjd4Z_92"
      },
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"MYKEY\")\n",
        "project = rf.workspace(\"detection-cwsyk\").project(\"hd-kouwf\")\n",
        "dataset = project.version(2).download(\"yolov5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upNt4p90mXKJ",
        "outputId": "d7778887-7ee0-44eb-d4e7-450b4473d6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/datasets/HD-2 to yolov5pytorch: 100% [87975462 / 87975462] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/HD-2 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7800/7800 [00:06<00:00, 1118.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwJcaoPGF4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e22e5ac-6980-403e-a723-ba205596761c"
      },
      "source": [
        "#after following the link above, recieve python code with these fields filled in\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"YOUR API KEY HERE\")\n",
        "#project = rf.workspace().project(\"YOUR PROJECT\")\n",
        "#dataset = project.version(\"YOUR VERSION\").download(\"yolov5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/American-Mushrooms-1 to yolov5pytorch: 100% [3866359 / 3866359] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/American-Mushrooms-1 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:00<00:00, 1046.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaFNnxLJbq4J",
        "outputId": "31abfef4-dd43-4ef7-ed1a-5ee3a274d3ef"
      },
      "source": [
        "!python train.py --patience 300 --img 640 --batch 16 --epochs 350 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/HD-2/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=350, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=300, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.1-251-gc23a441 Python-3.7.13 torch-1.11.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 18.0MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 126MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     29667  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7035811 parameters, 7035811 gradients\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/HD-2/train/labels' images and labels...2727 found, 0 missing, 0 empty, 0 corrupt: 100% 2727/2727 [00:02<00:00, 1199.38it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_1000_jpg.rf.05cca453ee36a1d51eb3f2b4a54c09eb.jpg: 7 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_560_jpg.rf.f2e96952d1e140d1b85b920447e3f8b4.jpg: 4 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_600_jpg.rf.47e7c47fa31a98f91a7391b5b55abb05.jpg: 6 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_640_jpg.rf.e5603dfc8e3446b18b61a4be99e940da.jpg: 7 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_680_jpg.rf.a32ad2b082d21a1c8bd5f590c9250c11.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_800_jpg.rf.d4ae1ced1985bf509c7f685792c9cf1a.jpg: 9 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_840_jpg.rf.97b17f7203ea56f1152e001fea83e50e.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_880_jpg.rf.a928f27ba8a0b820f2410fd4dd1e99e3.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/datasets/HD-2/train/images/deathCircle_video0_960_jpg.rf.c79edda2b665cf77e8f6b21b7838804a.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/HD-2/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (3.4GB ram): 100% 2727/2727 [00:15<00:00, 173.06it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/HD-2/valid/labels' images and labels...776 found, 0 missing, 0 empty, 0 corrupt: 100% 776/776 [00:01<00:00, 731.36it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/datasets/HD-2/valid/images/deathCircle_video0_520_jpg.rf.bbef4cfe999f36c2adf0ba84f1452ee5.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/datasets/HD-2/valid/images/deathCircle_video0_720_jpg.rf.4b22dad79d637471ac1da0d90dcbe87e.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/datasets/HD-2/valid/images/deathCircle_video0_760_jpg.rf.c600a137675a2abad3b7c5a4ae0794a3.jpg: 9 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/datasets/HD-2/valid/images/deathCircle_video0_920_jpg.rf.49ec5bc9d9aae68dff8417a8d529cb95.jpg: 8 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/HD-2/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (1.0GB ram): 100% 776/776 [00:04<00:00, 158.31it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.02 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 350 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     0/349        0G    0.1314    0.0621   0.05416       234       640:  25% 43/171 [16:45<49:52, 23.38s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 670, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 565, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 352, in train\n",
            "    pred = model(imgs)  # forward\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/yolov5/models/yolo.py\", line 135, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/yolov5/models/yolo.py\", line 158, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/yolov5/models/common.py\", line 158, in forward\n",
            "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/yolov5/models/common.py\", line 47, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 444, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('weights_3', 'zip', '/content/yolov5/runs/train/exp3/weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4Vu83ZpP07Rq",
        "outputId": "67150b03-5274-4d9c-de9d-02af0eee36aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5/weights_3.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('Detection for weight 3', 'zip', '/content/yolov5/runs/detect/exp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D9zMe8uSMxjo",
        "outputId": "4abb4a1d-216f-4682-eea8-d182b566d1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5/Detection for weight 3.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWjjiBcic3Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe988b5-26ce-447d-c8d7-711da653d6ec"
      },
      "source": [
        "!python detect.py --weights runs/train/exp3/weights/best.pt --img 640 --conf 0.1 --source {dataset.location}/test/images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp3/weights/best.pt'], source=/content/datasets/Human-Detection-2-1/test/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.1-250-g6adc53b Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7026307 parameters, 0 gradients\n",
            "image 1/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_1560_jpg.rf.f23cd91c7007439d6af5dc828c0d687c.jpg: 640x640 3 Bikers, 16 Pedestrians, Done. (0.013s)\n",
            "image 2/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_2010_jpg.rf.9a3548117a6f695bda678ee528d1890a.jpg: 640x640 3 Bikers, 14 Pedestrians, Done. (0.013s)\n",
            "image 3/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_2220_jpg.rf.856516a87f238e1a1b80820898422663.jpg: 640x640 15 Pedestrians, Done. (0.013s)\n",
            "image 4/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_2430_jpg.rf.6c32780df8188743a5ae4dec06d9fb44.jpg: 640x640 21 Pedestrians, Done. (0.014s)\n",
            "image 5/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_2820_jpg.rf.6c00c4c353693a8bdfad2aa47436cfb5.jpg: 640x640 2 Bikers, 19 Pedestrians, Done. (0.013s)\n",
            "image 6/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_3450_jpg.rf.5f7f47f617e3c21ac52894cf42ec5ea3.jpg: 640x640 3 Bikers, 12 Pedestrians, Done. (0.013s)\n",
            "image 7/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_3540_jpg.rf.44a081e29b6d36e624d82a308231e5f4.jpg: 640x640 1 Biker, 14 Pedestrians, Done. (0.013s)\n",
            "image 8/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_3630_jpg.rf.15b7080bbcb678ff07a69bcc38b56a12.jpg: 640x640 1 Biker, 14 Pedestrians, Done. (0.013s)\n",
            "image 9/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_3930_jpg.rf.40e0abc1322e69266af33975bd1be403.jpg: 640x640 2 Bikers, 8 Pedestrians, Done. (0.013s)\n",
            "image 10/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_4380_jpg.rf.4ada3de2457395cfe5451a3e5f5af2f0.jpg: 640x640 3 Bikers, 6 Pedestrians, Done. (0.013s)\n",
            "image 11/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5130_jpg.rf.6254e295f37b76bba1feda350acba1d7.jpg: 640x640 3 Bikers, 1 Car, 10 Pedestrians, Done. (0.011s)\n",
            "image 12/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5190_jpg.rf.02552299320f7e62f926d50bcee6d5c8.jpg: 640x640 5 Bikers, 1 Car, 7 Pedestrians, Done. (0.011s)\n",
            "image 13/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5280_jpg.rf.b8735d1da43f2e91617734e7ee9225a6.jpg: 640x640 4 Bikers, 1 Car, 7 Pedestrians, Done. (0.011s)\n",
            "image 14/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5430_jpg.rf.99ae41d9f08e31d5c29a3a9c611adc26.jpg: 640x640 5 Bikers, 13 Pedestrians, Done. (0.011s)\n",
            "image 15/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5610_jpg.rf.95ed9e0a971e65257f4d7509a00acd5b.jpg: 640x640 14 Pedestrians, Done. (0.012s)\n",
            "image 16/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5640_jpg.rf.2c870761b6760a66e90960ea5c9aed8d.jpg: 640x640 1 Biker, 1 Bus, 14 Pedestrians, Done. (0.011s)\n",
            "image 17/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_5940_jpg.rf.bca0e8bde3aff4777b2fe2a4f1ea7fc9.jpg: 640x640 2 Bikers, 1 Bus, 15 Pedestrians, Done. (0.011s)\n",
            "image 18/63 /content/datasets/Human-Detection-2-1/test/images/bookstore_video1_7560_jpg.rf.0cff61896fde1e7245b35887cc44f598.jpg: 640x640 3 Bikers, 9 Pedestrians, Done. (0.011s)\n",
            "image 19/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_1200_jpg.rf.8e534a8a25b368bb2d1c1376ac562d4e.jpg: 640x640 11 Pedestrians, Done. (0.015s)\n",
            "image 20/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_1530_jpg.rf.42cd30e45e0b67ca34f22d1eaafcb3aa.jpg: 640x640 1 Biker, 18 Pedestrians, Done. (0.011s)\n",
            "image 21/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_1560_jpg.rf.5edcd5c97ae760f223caa65f65c38572.jpg: 640x640 1 Biker, 16 Pedestrians, Done. (0.012s)\n",
            "image 22/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_1740_jpg.rf.a2f698eed56cbe8b080577259b569774.jpg: 640x640 1 Biker, 19 Pedestrians, Done. (0.011s)\n",
            "image 23/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_2400_jpg.rf.fd426cfd2dd95834216031404221e9c6.jpg: 640x640 12 Pedestrians, Done. (0.011s)\n",
            "image 24/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_2610_jpg.rf.66f7a59bd89ee7de6d7ac027be2d2d95.jpg: 640x640 1 Biker, 14 Pedestrians, Done. (0.011s)\n",
            "image 25/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_3420_jpg.rf.432f0aa754d8a2357f7a18373f83a0e6.jpg: 640x640 1 Biker, 17 Pedestrians, Done. (0.014s)\n",
            "image 26/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_3570_jpg.rf.90b5dcd98ef0e713210be77a3bf6a421.jpg: 640x640 1 Biker, 20 Pedestrians, Done. (0.012s)\n",
            "image 27/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_3660_jpg.rf.01b1d97ed549bde2aa92d5af736492a9.jpg: 640x640 1 Biker, 18 Pedestrians, Done. (0.011s)\n",
            "image 28/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_3750_jpg.rf.7c2a3e29a54dd17d400792b2c0ed4f67.jpg: 640x640 1 Biker, 18 Pedestrians, Done. (0.011s)\n",
            "image 29/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_4350_jpg.rf.68178a4d8d5df8771c5162612132d063.jpg: 640x640 1 Biker, 12 Pedestrians, Done. (0.011s)\n",
            "image 30/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_4590_jpg.rf.0c12cd45bceaeeafe4ad9c9e4f398d4e.jpg: 640x640 3 Bikers, 12 Pedestrians, Done. (0.011s)\n",
            "image 31/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_4650_jpg.rf.e94875dfe9118b8edb0126572e74a77b.jpg: 640x640 3 Bikers, 13 Pedestrians, Done. (0.011s)\n",
            "image 32/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_4920_jpg.rf.64af6a2a9a2afbec96f31ab8388c3aa6.jpg: 640x640 4 Bikers, 20 Pedestrians, Done. (0.011s)\n",
            "image 33/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_5220_jpg.rf.9618f3090076f507af89921938b822a6.jpg: 640x640 2 Bikers, 21 Pedestrians, Done. (0.012s)\n",
            "image 34/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_5490_jpg.rf.5c6c20774d2b47192d603316f2c25bd6.jpg: 640x640 20 Pedestrians, Done. (0.011s)\n",
            "image 35/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_5640_jpg.rf.1e2419f8d1fcc3cd2488961395c4ed06.jpg: 640x640 16 Pedestrians, Done. (0.011s)\n",
            "image 36/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_5760_jpg.rf.4cd53cdfc1f3f069ddb75ddbcf108b83.jpg: 640x640 11 Pedestrians, Done. (0.011s)\n",
            "image 37/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_5970_jpg.rf.fcf0f47f7aafe35dd88d35bb0d1b2c83.jpg: 640x640 15 Pedestrians, Done. (0.010s)\n",
            "image 38/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_6090_jpg.rf.b64ab5a9d7dd50efb9046d7db087bab5.jpg: 640x640 15 Pedestrians, Done. (0.012s)\n",
            "image 39/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_6270_jpg.rf.d2b682d47109daa3ca63afdc5ff23c8d.jpg: 640x640 1 Biker, 18 Pedestrians, Done. (0.011s)\n",
            "image 40/63 /content/datasets/Human-Detection-2-1/test/images/coupa_video3_6450_jpg.rf.edaea994d27c8f88fa67a7bb3f152567.jpg: 640x640 2 Bikers, 17 Pedestrians, Done. (0.011s)\n",
            "image 41/63 /content/datasets/Human-Detection-2-1/test/images/deathCircle_video4_180_jpg.rf.0081d4405d4f216b3fb33735c3953628.jpg: 640x640 36 Bikers, 8 Carts, 4 Pedestrians, Done. (0.011s)\n",
            "image 42/63 /content/datasets/Human-Detection-2-1/test/images/deathCircle_video4_60_jpg.rf.50440c91f3be47a54120fb326e3782a7.jpg: 640x640 32 Bikers, 17 Carts, 7 Pedestrians, Done. (0.011s)\n",
            "image 43/63 /content/datasets/Human-Detection-2-1/test/images/gates_video5_480_jpg.rf.a433cb58b857aeea28aae5370ed9a6ce.jpg: 640x640 4 Bikers, 5 Pedestrians, Done. (0.012s)\n",
            "image 44/63 /content/datasets/Human-Detection-2-1/test/images/gates_video5_720_jpg.rf.9f5d535d83d8c7bc87063eb86160bc28.jpg: 640x640 5 Bikers, 7 Pedestrians, Done. (0.011s)\n",
            "image 45/63 /content/datasets/Human-Detection-2-1/test/images/gates_video5_810_jpg.rf.ebf87270c8d3648c893871ec84a66fb9.jpg: 640x640 5 Bikers, 7 Pedestrians, Done. (0.011s)\n",
            "image 46/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_1560_jpg.rf.6eb0b27473ce308f58301cdbd87b77f4.jpg: 640x640 1 Biker, 3 Pedestrians, Done. (0.011s)\n",
            "image 47/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_2760_jpg.rf.3329af7b1c5fd38cc0f799bcf00a394a.jpg: 640x640 3 Bikers, Done. (0.011s)\n",
            "image 48/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_3840_jpg.rf.89246454ef7faf2735a85695d6234ecd.jpg: 640x640 2 Bikers, 3 Pedestrians, Done. (0.011s)\n",
            "image 49/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_3960_jpg.rf.9ee7973207dab0a24b2952e3ca2e2252.jpg: 640x640 2 Bikers, 2 Pedestrians, Done. (0.011s)\n",
            "image 50/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_4380_jpg.rf.c952198f503da04577c5892933e728fb.jpg: 640x640 1 Biker, 1 Pedestrian, Done. (0.012s)\n",
            "image 51/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_4770_jpg.rf.432696a0d84c8fa36e5df30c0ae998a8.jpg: 640x640 3 Bikers, Done. (0.011s)\n",
            "image 52/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_4950_jpg.rf.c4fe2cbbcbc9c03f039540d029072197.jpg: 640x640 1 Biker, Done. (0.011s)\n",
            "image 53/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_5430_jpg.rf.287a0114e9d753c5c521358982c214d2.jpg: 640x640 1 Biker, 2 Pedestrians, Done. (0.011s)\n",
            "image 54/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_5730_jpg.rf.5636c966cf79bee7910aad06b994e10d.jpg: 640x640 2 Bikers, 6 Pedestrians, Done. (0.012s)\n",
            "image 55/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_5790_jpg.rf.64b6db402866ce809fd3e54a96367bf9.jpg: 640x640 2 Bikers, 6 Pedestrians, Done. (0.011s)\n",
            "image 56/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_5880_jpg.rf.74878a386accdcab8ed5df14f1927797.jpg: 640x640 3 Bikers, 9 Pedestrians, Done. (0.013s)\n",
            "image 57/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_6750_jpg.rf.c530189ef623872538944ed3ff85f06a.jpg: 640x640 1 Biker, 8 Pedestrians, Done. (0.011s)\n",
            "image 58/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7200_jpg.rf.48305d577e5fbe0c6230cc50be703011.jpg: 640x640 2 Bikers, 3 Pedestrians, Done. (0.011s)\n",
            "image 59/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7320_jpg.rf.49041d38bae714e7f220211b1b1c4e12.jpg: 640x640 3 Bikers, 2 Pedestrians, Done. (0.011s)\n",
            "image 60/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7350_jpg.rf.d145a5fe9dcb173d2bdb86a6ca47ae46.jpg: 640x640 2 Bikers, 2 Pedestrians, Done. (0.011s)\n",
            "image 61/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7410_jpg.rf.24a25dd1aed86c6d8aa7871d0253c438.jpg: 640x640 1 Biker, 2 Pedestrians, Done. (0.011s)\n",
            "image 62/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7440_jpg.rf.4150d5bec2d0217106ef520bcdf19ef6.jpg: 640x640 1 Biker, 2 Pedestrians, Done. (0.011s)\n",
            "image 63/63 /content/datasets/Human-Detection-2-1/test/images/little_video1_7620_jpg.rf.e3bf99811214acaae1fc8f9aa6d288ee.jpg: 640x640 3 Bikers, 4 Pedestrians, Done. (0.016s)\n",
            "Speed: 0.5ms pre-process, 11.7ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbUn4_b9GCKO"
      },
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7iiObB2WCMh6",
        "outputId": "5c2c1e8a-a857-4284-d627-42f99724c9e8"
      },
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bd6fc902-d327-4332-bf86-0f81058a7d7f\", \"best.pt\", 14358689)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}